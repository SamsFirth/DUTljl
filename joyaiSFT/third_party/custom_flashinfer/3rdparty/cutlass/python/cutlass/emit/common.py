"""
Common utilities for emitting CUTLASS kernels
"""
import cutlass
_AUTOGEN_STR = f'This file was automatically generated by the CUTLASS {cutlass.__version__} Python interface (https://github.com/nvidia/cutlass/python)'
_CSTYLE_AUTOGEN_COMMENT = f'// {_AUTOGEN_STR}\n'
_PYSTYLE_AUTOGEN_COMMENT = f'# {_AUTOGEN_STR}\n'
_CUTLASS_KERNEL_ARGS_2x = '\n  typename DeviceKernel::Arguments arguments {\n      cutlass::gemm::GemmUniversalMode::kGemm,\n      {M, N, K},                                        // problem size\n      1,\n      {alpha, beta},\n      A, B, C, D,\n      0, 0, 0, 0,                                       // batch strides\n      DeviceKernel::LayoutA::packed({M, K}).stride(0),  // lda\n      DeviceKernel::LayoutB::packed({K, N}).stride(0),  // ldb\n      DeviceKernel::LayoutC::packed({M, N}).stride(0),  // ldc\n      DeviceKernel::LayoutC::packed({M, N}).stride(0)   // ldd\n  };\n'
_CUTLASS_KERNEL_ARGS_2x_STREAM_K = '\n  typename DeviceKernel::Arguments arguments {\n      cutlass::gemm::GemmUniversalMode::kGemm,\n      {M, N, K},                                        // problem size\n      1,\n      {alpha, beta},\n      A, B, C, D,\n      0, 0, 0, 0,                                       // batch strides\n      DeviceKernel::LayoutA::packed({M, K}).stride(0),  // lda\n      DeviceKernel::LayoutB::packed({K, N}).stride(0),  // ldb\n      DeviceKernel::LayoutC::packed({M, N}).stride(0),  // ldc\n      DeviceKernel::LayoutC::packed({M, N}).stride(0),  // ldd\n      -1                                                // avail_sms\n  };\n'
_CUTLASS_KERNEL_RUN_GEMM_2x = '\nusing ElementCompute = typename DeviceKernel::EpilogueOutputOp::ElementCompute;\n\ncutlass::Status ${name}_kernel_run(int M, int N, int K,\n                        const DeviceKernel::ElementA* A, const DeviceKernel::ElementB* B, const DeviceKernel::ElementC* C, DeviceKernel::ElementC* D,\n                        ElementCompute alpha, ElementCompute beta) {\n  ${args}\n  size_t workspace_size = DeviceKernel::get_workspace_size(arguments);\n  cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);\n\n  DeviceKernel gemm_op;\n  cutlass::Status status = gemm_op.initialize(arguments,\n                                              workspace.get(),\n                                              nullptr);     // CUDA stream\n\n  if (status != cutlass::Status::kSuccess) {\n    return status;\n  }\n\n  status = gemm_op();\n  return status;\n}\n'
_CUTLASS_KERNEL_RUN_GEMM_3x = '\nusing StrideA = typename DeviceKernel::GemmKernel::StrideA;\nusing StrideB = typename DeviceKernel::GemmKernel::StrideB;\nusing StrideC = typename DeviceKernel::GemmKernel::StrideC;\nusing StrideD = typename DeviceKernel::GemmKernel::StrideD;\n\nusing ElementCompute = typename DeviceKernel::EpilogueOutputOp::ElementCompute;\n\ncutlass::Status ${name}_kernel_run(\n        int M, int N, int K, int L,\n        const DeviceKernel::ElementA* A, const DeviceKernel::ElementB* B, const DeviceKernel::ElementC* C, DeviceKernel::ElementC* D,\n        ElementCompute alpha, ElementCompute beta, const cutlass::KernelHardwareInfo& hw_info) {\n\n  typename DeviceKernel::Arguments arguments{\n      cutlass::gemm::GemmUniversalMode::kGemm,\n      {M, N, K, L},                                                              // problem size\n      {\n        A,                                                                         // ptrA\n        cutlass::make_cute_packed_stride(StrideA{}, cute::make_shape(M, K, L)),    // stride A\n        B,                                                                         // ptrB\n        cutlass::make_cute_packed_stride(StrideB{}, cute::make_shape(N, K, L)),    // stride B\n      },\n      {\n        {alpha, beta},\n        C,                                                                       // ptrC\n        cutlass::make_cute_packed_stride(StrideC{}, cute::make_shape(M, N, L)),  // stride C\n        D,                                                                       // ptrD\n        cutlass::make_cute_packed_stride(StrideD{}, cute::make_shape(M, N, L)),  // stride D\n      },\n      hw_info\n  };\n\n  size_t workspace_size = DeviceKernel::get_workspace_size(arguments);\n  cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);\n\n  DeviceKernel gemm_op;\n  cutlass::Status status = gemm_op.run(arguments,\n                                       workspace.get(),\n                                       nullptr);     // CUDA stream\n\n  return status;\n}\n'
_CUTLASS_KERNEL_RUN_GROUPED_GEMM_2x = '\nusing ElementCompute = typename DeviceKernel::EpilogueOutputOp::ElementCompute;\n\nint threadblock_count = DeviceKernel::sufficient();\n\ncutlass::Status ${name}_kernel_run(int problem_count, cutlass::gemm::GemmCoord* problem_sizes,\n                        DeviceKernel::ElementA** A, DeviceKernel::ElementB** B, DeviceKernel::ElementC** C, DeviceKernel::ElementC** D,\n                        int64_t* lda, int64_t* ldb, int64_t* ldc, int64_t* ldd,\n                        ElementCompute alpha, ElementCompute beta) {\n\n  typename DeviceKernel::Arguments arguments {\n    problem_sizes,\n    problem_count,\n    threadblock_count,\n    {alpha, beta},\n    A, B, C, D,\n    lda, ldb, ldc, ldd\n  };\n\n  size_t workspace_size = DeviceKernel::get_workspace_size(arguments);\n  cutlass::device_memory::allocation<uint8_t> workspace(workspace_size);\n\n  DeviceKernel gemm_op;\n  cutlass::Status status = gemm_op.initialize(arguments,\n                                              workspace.get(),\n                                              nullptr);     // CUDA stream\n\n  if (status != cutlass::Status::kSuccess) {\n    return status;\n  }\n\n  status = gemm_op();\n  return status;\n}\n'
_CUTLASS_KERNEL_RUN_CONV2D_2x = '\n\nusing UnderlyingKernel = typename DeviceKernel::UnderlyingKernel;\nnamespace {\nusing TensorRefA = typename UnderlyingKernel::TensorRefA;\nusing TensorRefB = typename UnderlyingKernel::TensorRefB;\nusing TensorRefC = typename UnderlyingKernel::TensorRefC;\nusing ElementCompute = typename UnderlyingKernel::EpilogueOutputOp::ElementCompute;\n}\n\ntemplate<typename TensorRef, typename Element>\nTensorRef get_tensor_ref(cutlass::Tensor4DCoord tensor_coord, Element* ptr){\n  cutlass::layout::TensorNHWC layout = cutlass::layout::TensorNHWC::packed(tensor_coord);\n  TensorRef tensor_ref(ptr, layout);\n  return tensor_ref;\n}\n\ncutlass::Status ${name}_kernel_run(cutlass::conv::Conv2dProblemSize* problem_size,\n                        UnderlyingKernel::ElementA* A, UnderlyingKernel::ElementB* B,\n                        UnderlyingKernel::ElementC* C, UnderlyingKernel::ElementC* D,\n                        ElementCompute alpha, ElementCompute beta, std::string split_k_mode,\n                        cudaStream_t stream, int device_id=0) {\n  // create the tensor references\n  cutlass::Tensor4DCoord tensor_coord_A = cutlass::conv::implicit_gemm_tensor_a_extent(\n    cutlass::conv::Operator::k${conv_kind_name}, *problem_size\n  );\n  cutlass::Tensor4DCoord tensor_coord_B = cutlass::conv::implicit_gemm_tensor_b_extent(\n    cutlass::conv::Operator::k${conv_kind_name}, *problem_size\n  );\n  cutlass::Tensor4DCoord tensor_coord_C = cutlass::conv::implicit_gemm_tensor_c_extent(\n    cutlass::conv::Operator::k${conv_kind_name}, *problem_size\n  );\n\n  TensorRefA tensor_ref_A = get_tensor_ref<TensorRefA, UnderlyingKernel::ElementA>(tensor_coord_A, A);\n  TensorRefB tensor_ref_B = get_tensor_ref<TensorRefB, UnderlyingKernel::ElementB>(tensor_coord_B, B);\n  TensorRefC tensor_ref_C = get_tensor_ref<TensorRefC, UnderlyingKernel::ElementC>(tensor_coord_C, C);\n  TensorRefC tensor_ref_D = get_tensor_ref<TensorRefC, UnderlyingKernel::ElementC>(tensor_coord_C, D);\n\n  cutlass::conv::SplitKMode mode;\n  if (split_k_mode == "serial") {\n    mode = cutlass::conv::SplitKMode::kSerial;\n  } else if (split_k_mode == "parallel") {\n    mode = cutlass::conv::SplitKMode::kParallel;\n  } else {\n    throw std::runtime_error("Invalid split_k_mode: " + split_k_mode);\n  }\n\n  typename DeviceKernel::Arguments arguments{\n    *problem_size,\n    tensor_ref_A,\n    tensor_ref_B,\n    tensor_ref_C,\n    tensor_ref_D,\n    {alpha, beta},\n    mode\n  };\n\n  DeviceKernel implicit_gemm_op;\n\n  size_t workspace_size = implicit_gemm_op.get_workspace_size(arguments);\n\n  void* workspace_ptr = device_memory_allocation(workspace_size, device_id);\n\n  cutlass::Status status = implicit_gemm_op.can_implement(arguments);\n  if (status != cutlass::Status::kSuccess) {\n    return status;\n  }\n\n  status = implicit_gemm_op.initialize(arguments, workspace_ptr, stream);\n  if (status != cutlass::Status::kSuccess) {\n    return status;\n  }\n\n  //\n  // Launch initialized CUTLASS kernel\n  //\n  status = implicit_gemm_op(stream);\n\n  return status;\n}\n'